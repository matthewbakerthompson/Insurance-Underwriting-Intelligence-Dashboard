{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance Risk Analysis Notebook\n",
    "\n",
    "This notebook demonstrates data analysis and modeling techniques for insurance underwriting risk assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Add parent directory to path to import project modules\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Sample Insurance Data\n",
    "\n",
    "For demonstration purposes, we'll generate synthetic insurance data that mimics real-world underwriting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate property data\n",
    "def generate_property_data(n_samples=1000):\n",
    "    property_data = pd.DataFrame({\n",
    "        'property_id': [f'P{i:04d}' for i in range(1, n_samples + 1)],\n",
    "        'property_type': np.random.choice(['Residential', 'Commercial', 'Industrial'], n_samples),\n",
    "        'property_value': np.random.uniform(100000, 1000000, n_samples),\n",
    "        'property_age': np.random.randint(1, 50, n_samples),\n",
    "        'property_size': np.random.uniform(1000, 10000, n_samples),\n",
    "        'flood_risk_score': np.random.uniform(0, 10, n_samples),\n",
    "        'fire_risk_score': np.random.uniform(0, 10, n_samples),\n",
    "        'location_risk': np.random.choice(['Low', 'Medium', 'High'], n_samples),\n",
    "    })\n",
    "    \n",
    "    # Create a more realistic target variable with dependencies on features\n",
    "    # Higher risk scores and property age increase claim probability\n",
    "    logits = (\n",
    "        0.05 * property_data['property_value'] / 100000 +\n",
    "        0.2 * property_data['property_age'] +\n",
    "        0.4 * property_data['flood_risk_score'] +\n",
    "        0.3 * property_data['fire_risk_score'] +\n",
    "        (property_data['location_risk'] == 'High') * 2 +\n",
    "        (property_data['location_risk'] == 'Medium') * 1\n",
    "    )\n",
    "    \n",
    "    # Normalize and convert to probability\n",
    "    logits = (logits - logits.mean()) / logits.std()\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    \n",
    "    # Generate binary outcome\n",
    "    property_data['claim_filed'] = (np.random.random(n_samples) < probs).astype(int)\n",
    "    \n",
    "    return property_data\n",
    "\n",
    "# Generate the data\n",
    "property_data = generate_property_data(1000)\n",
    "\n",
    "# Display the first few rows\n",
    "property_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "Let's explore the data to understand the relationships between different features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic statistics\n",
    "property_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Distribution of property types\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='property_type', data=property_data)\n",
    "plt.title('Distribution of Property Types')\n",
    "plt.xlabel('Property Type')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Distribution of claims by property type\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='property_type', hue='claim_filed', data=property_data)\n",
    "plt.title('Claims by Property Type')\n",
    "plt.xlabel('Property Type')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(['No Claim', 'Claim Filed'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Claim rate by property type\n",
    "claim_rates = property_data.groupby('property_type')['claim_filed'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=claim_rates.index, y=claim_rates.values)\n",
    "plt.title('Claim Rate by Property Type')\n",
    "plt.xlabel('Property Type')\n",
    "plt.ylabel('Claim Rate')\n",
    "plt.ylim(0, 0.5)  # Adjust as needed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Correlation between numeric features\n",
    "numeric_cols = ['property_value', 'property_age', 'property_size', 'flood_risk_score', 'fire_risk_score', 'claim_filed']\n",
    "corr = property_data[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Between Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Distribution of risk scores\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(property_data, x='flood_risk_score', hue='claim_filed', kde=True, bins=20)\n",
    "plt.title('Flood Risk Score Distribution')\n",
    "plt.xlabel('Flood Risk Score')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(property_data, x='fire_risk_score', hue='claim_filed', kde=True, bins=20)\n",
    "plt.title('Fire Risk Score Distribution')\n",
    "plt.xlabel('Fire Risk Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Relationship between property age and claim rate\n",
    "# Create age bins\n",
    "property_data['age_bin'] = pd.cut(property_data['property_age'], bins=[0, 10, 20, 30, 40, 50], labels=['0-10', '11-20', '21-30', '31-40', '41-50'])\n",
    "\n",
    "# Calculate claim rate by age bin\n",
    "age_claim_rates = property_data.groupby('age_bin')['claim_filed'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=age_claim_rates.index, y=age_claim_rates.values)\n",
    "plt.title('Claim Rate by Property Age')\n",
    "plt.xlabel('Property Age (years)')\n",
    "plt.ylabel('Claim Rate')\n",
    "plt.ylim(0, 0.5)  # Adjust as needed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Let's create some additional features that might be useful for predicting insurance claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a copy of the data for feature engineering\n",
    "data_engineered = property_data.copy()\n",
    "\n",
    "# Create combined risk score\n",
    "data_engineered['combined_risk_score'] = (data_engineered['flood_risk_score'] + data_engineered['fire_risk_score']) / 2\n",
    "\n",
    "# Create value per square foot\n",
    "data_engineered['value_per_sqft'] = data_engineered['property_value'] / data_engineered['property_size']\n",
    "\n",
    "# Create age-related features\n",
    "data_engineered['is_new_property'] = (data_engineered['property_age'] < 5).astype(int)\n",
    "data_engineered['is_old_property'] = (data_engineered['property_age'] > 30).astype(int)\n",
    "\n",
    "# Create interaction features\n",
    "data_engineered['age_x_flood_risk'] = data_engineered['property_age'] * data_engineered['flood_risk_score']\n",
    "data_engineered['age_x_fire_risk'] = data_engineered['property_age'] * data_engineered['fire_risk_score']\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "data_engineered = pd.get_dummies(data_engineered, columns=['property_type', 'location_risk'], drop_first=False)\n",
    "\n",
    "# Display the engineered features\n",
    "data_engineered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "Now let's train machine learning models to predict insurance claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare data for modeling\n",
    "# Exclude non-feature columns\n",
    "exclude_cols = ['property_id', 'claim_filed', 'age_bin']\n",
    "feature_cols = [col for col in data_engineered.columns if col not in exclude_cols]\n",
    "\n",
    "X = data_engineered[feature_cols].values\n",
    "y = data_engineered['claim_filed'].values\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train multiple models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'model': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {results[name]['accuracy']:.4f}\")\n",
    "    print(f\"  ROC AUC: {results[name]['roc_auc']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare model performance\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "model_comparison = pd.DataFrame(index=models.keys(), columns=metrics)\n",
    "\n",
    "for name in models.keys():\n",
    "    for metric in metrics:\n",
    "        model_comparison.loc[name, metric] = results[name][metric]\n",
    "\n",
    "model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "model_comparison.plot(kind='bar', figsize=(12, 8))\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, result in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])\n",
    "    roc_auc = result['roc_auc']\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Interpretation\n",
    "\n",
    "Let's analyze the best model to understand which features are most important for predicting insurance claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Determine the best model\n",
    "best_model_name = model_comparison['roc_auc'].idxmax()\n",
    "best_model = results[best_model_name]['model']\n",
    "print(f\"Best model: {best_model_name} with ROC AUC: {model_comparison.loc[best_model_name, 'roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance for the best model\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Get feature importances\n",
    "    importances = best_model.feature_importances_\n",
    "    \n",
    "    # Create a DataFrame for visualization\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot top 15 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "    plt.title(f'Top 15 Feature Importances ({best_model_name})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importances not available for this model type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# SHAP values for model interpretation\n",
    "try:\n",
    "    # Create a SHAP explainer\n",
    "    if isinstance(best_model, (RandomForestClassifier, GradientBoostingClassifier, xgb.XGBClassifier)):\n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "        \n",
    "        # Calculate SHAP values for a subset of test data\n",
    "        shap_values = explainer.shap_values(X_test[:100])\n",
    "        \n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_test[:100], feature_names=feature_cols)\n",
    "        plt.title(f'SHAP Summary Plot ({best_model_name})')\n",
    "        plt.show()\n",
    "        \n",
    "        # Dependence plots for top features\n",
    "        if feature_importance is not None:\n",
    "            top_features = feature_importance.head(3)['Feature'].values\n",
    "            for feature in top_features:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                feature_idx = feature_cols.index(feature)\n",
    "                shap.dependence_plot(feature_idx, shap_values, X_test[:100], feature_names=feature_cols)\n",
    "                plt.title(f'SHAP Dependence Plot for {feature}')\n",
    "                plt.show()\n",
    "    else:\n",
    "        print(\"SHAP analysis not implemented for this model type.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in SHAP analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Business Insights and Recommendations\n",
    "\n",
    "Based on our analysis, let's summarize key insights and recommendations for underwriting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights:\n",
    "\n",
    "1. **Risk Factors**: The most important predictors of insurance claims are [highlight top 3-5 features based on importance].\n",
    "\n",
    "2. **Property Types**: [Describe differences in claim rates between property types].\n",
    "\n",
    "3. **Age Impact**: [Describe how property age affects claim probability].\n",
    "\n",
    "4. **Risk Scores**: [Describe how flood and fire risk scores correlate with claims].\n",
    "\n",
    "### Recommendations for Underwriting:\n",
    "\n",
    "1. **Risk Assessment**: Prioritize evaluation of [top risk factors] when assessing new policies.\n",
    "\n",
    "2. **Premium Adjustment**: Consider adjusting premiums based on [specific risk factors].\n",
    "\n",
    "3. **Risk Mitigation**: For high-risk properties, recommend specific risk mitigation measures related to [relevant risk factors].\n",
    "\n",
    "4. **Data Collection**: Improve collection of data related to [important features] to enhance future risk models.\n",
    "\n",
    "5. **Model Implementation**: Implement the [best model] in the underwriting process to improve risk assessment accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "1. **Model Refinement**: Further tune hyperparameters and explore ensemble methods to improve model performance.\n",
    "\n",
    "2. **Additional Data**: Integrate additional third-party data sources such as weather patterns, crime statistics, and economic indicators.\n",
    "\n",
    "3. **Deployment Strategy**: Develop an API for real-time risk scoring in the underwriting process.\n",
    "\n",
    "4. **Monitoring Plan**: Establish a monitoring framework to track model performance over time and detect concept drift.\n",
    "\n",
    "5. **Feedback Loop**: Create a mechanism to incorporate underwriter feedback to continuously improve the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
